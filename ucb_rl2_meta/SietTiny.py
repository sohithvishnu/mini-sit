# ------------------------------------------------------------------------------------
# SiT: Symmetry-Invariant Transformers for Generalisation in Reinforcement Learning
# ------------------------------------------------------------------------------------
#
# Copyright (C) 2024 by Matthias Weissenbacher
#
# This file is part of the implementation for the ICML 2024 paper titled
# "SiT: Symmetry-Invariant Transformers for Generalisation in Reinforcement Learning".
# The implementation details are confidential and proprietary.
#
# ------------------------------------------------------------------------------------
# Abstract:
# ------------------------------------------------------------------------------------
# A major open challenge in reinforcement learning (RL) is the effective deployment
# of a trained policy to out-of-distribution data and semantically-similar environments.
# To overcome these limitations, we propose an invariant as well as equivariant scalable
# Transformer model that respects various local and global symmetry transformations
# of the input data. We present a symmetry-preserving neural network attention layer
# by adapting the self-attention mechanism to maintain graph symmetries, referred to
# as Graph Symmetric Attention mechanism (GSA). Our model leverages the interplay
# of local vs. global information to attain inherent out-of-distribution generalization.
# The invariant and equivariant latent representations are then used as a starting point
# for subsequent policy and value networks. Building on vision transformers, we
# demonstrate improved generalization using our approach on MiniGrid and Procgen
# environments.
#
# ------------------------------------------------------------------------------------
# License:
# ------------------------------------------------------------------------------------
# This software is provided "as is", without warranty of any kind, express or implied,
# including but not limited to the warranties of merchantability, fitness for a
# particular purpose and noninfringement. In no event shall the authors or copyright
# holders be liable for any claim, damages or other liability, whether in an action
# of contract, tort or otherwise, arising from, out of or in connection with the
# software or the use or other dealings in the software.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# ------------------------------------------------------------------------------------


import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import warnings

from functools import partial
from itertools import repeat
import collections.abc

from torch.nn.init import _calculate_fan_in_and_fan_out

from ucb_rl2_meta.Siet import Block_base


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        return x


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    if mode == 'fan_in':
        denom = fan_in
    elif mode == 'fan_out':
        denom = fan_out
    elif mode == 'fan_avg':
        denom = (fan_in + fan_out) / 2

    variance = scale / denom

    if distribution == "truncated_normal":
        # constant is stddev of standard normal truncated to (-2, 2)
        trunc_normal_(tensor, std=math.sqrt(variance) / .87962566103423978)
    elif distribution == "normal":
        tensor.normal_(std=math.sqrt(variance))
    elif distribution == "uniform":
        bound = math.sqrt(3 * variance)
        tensor.uniform_(-bound, bound)
    else:
        raise ValueError(f"invalid distribution {distribution}")


def lecun_normal_(tensor):
    variance_scaling_(tensor, mode='fan_in', distribution='truncated_normal')


# --------------------------------  Siet model code  ---------------------------------------------
# -----------------------------------------------------------------------------

def rot_grid_to_idx(dim):
    # Initialize indices and angles arrays
    n = dim ** 2
    idxs = torch.zeros((3 * n ** 2))
    angles = torch.zeros((3 * n ** 2))
    N = torch.arange(0, n).reshape(dim, dim)
    N2 = torch.arange(0, n ** 2).reshape(n, n)

    ci = 0  # Current index for filling idxs and angles

    # Iterate over each position in the dim x dim grid
    for i0 in range(dim):
        for j0 in range(dim):
            I = i0 * dim + j0  # Convert 2D index to 1D index for vertex i0, j0

            # Iterate over each position for the second vertex
            for i in range(dim):
                for j in range(dim):
                    J = i * dim + j  # Convert 2D index to 1D index for vertex i, j
                    # Define the rotation matrix for 90 degree right rotation
                    right90 = torch.tensor([[0, 1], [-1, 0]]).float()
                    vec = right90 @ torch.tensor([i - i0, j - j0]).float()

                    if (i, j) == (i0, j0):
                        # No rotation needed if same vertex
                        idxs[ci:ci + 3] = N2[N[i0, j0], N[i, j]]
                        angles[ci:ci + 3] = 1 / 3
                        ci += 3
                    else:
                        # Normalize the vector and correct direction for edge
                        vecn = torch.round(vec / torch.norm(vec))
                        if torch.equal(vecn, torch.tensor([1, 1]).float()):
                            vecn = torch.tensor([1, 0])
                        elif torch.equal(vecn, torch.tensor([1, -1]).float()):
                            vecn = torch.tensor([0, -1])
                        elif torch.equal(vecn, torch.tensor([-1, 1]).float()):
                            vecn = torch.tensor([0, 1])
                        elif torch.equal(vecn, torch.tensor([-1, -1]).float()):
                            vecn = torch.tensor([-1, 0])

                        vect = vecn + torch.tensor([i, j])
                        idx = vect.long()

                        try:
                            if -1 in torch.sign(idx):
                                raise Exception('no negative indexing here')
                            # Indices for triangular path in the graph
                            tri_start = N[i, j]
                            tri_middle = N[idx[0], idx[1]]
                            tri_end = N[i0, j0]

                            idxs[ci] = N2[tri_start, tri_middle]
                            idxs[ci + 1] = N2[tri_middle, tri_end]
                            idxs[ci + 2] = N2[N[i0, j0], N[i, j]]

                            vecf = (torch.tensor([i0, j0]) - vect).float()
                            vec_i = torch.tensor([i - i0, j - j0]).float()

                            # Calculate angles for preserving rotation symmetry
                            angle1 = torch.acos((vec / torch.norm(vec)) @ (vecf / torch.norm(vecf))) * 1 / math.pi
                            angle2 = torch.acos((vec_i / torch.norm(vec_i)) @ (vecf / torch.norm(vecf))) * 1 / math.pi

                            angles[ci] = angle1 / 2
                            angles[ci + 1] = angle2 / 2
                            angles[ci + 2] = 1 - angle1 / 2 - angle2 / 2
                            ci += 3
                        except:
                            # Fallback to default case if indexing is out of bounds
                            idxs[ci:ci + 3] = N2[N[i0, j0], N[i, j]]
                            angles[ci:ci + 3] = 1 / 3
                            ci += 3

    # Normalize and unique the angles
    dist = torch.round(100000 * angles)
    angles = torch.unique(angles.flatten(), dim=-1)
    unique = torch.unique(dist.flatten(), dim=-1)

    dimu = unique.shape[0]
    idxs_angles = unique.shape[0] * torch.ones(dist.shape)

    # Create a mapping of distances to their unique indices
    for i in range(dimu):
        mask = (dist == unique[i])
        idxs_angles[mask] = i

    return idxs.long(), idxs_angles.long(), angles, dimu


# -------------------------------  Siet model supplementary code ----------------------------------------------
# -----------------------------------------------------------------------------


class Rotation_Symmetry_Break(nn.Module):
    """
    A custom neural network layer that explicitly handles and breaks flip invariance
    (and preserved rotation symmetry) in input data, using learned angular relationships.
    """

    def __init__(self, num_patches, num_heads):
        """
        Initialize the layer with the number of patches and the number of attention heads.

        Args:
            num_patches (int): Total number of patches in the input.
            num_heads (int): Number of attention heads in the transformer model.
        """
        super().__init__()
        self.num_patches = num_patches  # Store the number of patches

        # Generate indices and angles for rotation symmetry handling
        idxs, idxs_angles, angles, dim_a = rot_grid_to_idx(int(math.sqrt(num_patches)))
        self.idxs = idxs  # Indices for selecting patches
        # Parameter matrix for angles, learning different angle weights per head
        self.angles = nn.Parameter(torch.Tensor(num_heads, dim_a), requires_grad=True)
        self.idxs_angles = idxs_angles  # Indices to map each angle to its corresponding angle in 'angles'

        # Initialize angle parameters using Kaiming uniform initialization
        nn.init.kaiming_uniform_(self.angles, a=math.sqrt(5))

    def forward(self, x):
        """
        Define the forward pass of the module, applying rotational transformations
        and summing contributions based on learned angles.

        Args:
            x (Tensor): Input tensor of shape (batch_size, num_heads, num_patches, num_patches)

        Returns:
            Tensor: Output tensor after applying rotation symmetry breaking transformations.
        """
        bs, heads, ps, _ = x.size()  # Extract dimensions from input tensor

        # Use indexed selection to rearrange patches according to precomputed indices,
        # maintaining rotation and translation invariance but breaking mirror symmetry.
        y = x.flatten(-2)[:, :, self.idxs]

        # Select and apply angle parameters based on the angle indices, adjusted for the current device
        angles = torch.index_select(self.angles, 1, self.idxs_angles.flatten().to(x.device))

        # Apply the angle weights to the selected patches and sum across the last dimension
        y = (y * angles.view(1, heads, 3 * self.num_patches ** 2)).reshape((bs, heads, ps, ps, 3)).sum(-1)

        return y.reshape(x.shape)  # Reshape back to the original input shape for compatibility


# ------------------------------- supplementary GSA layer code ----------------------------------------------
# -----------------------------------------------------------------------------


def grid_dist_to_idx(dim=5):
    """
    Compute the index matrix for a global graph self-attention (GSA) layer based on the distances from the center of a grid.

    Args:
        dim (int): Dimension of the square grid. It should be an odd number to ensure a central point.

    Returns:
        Tuple[Tensor, int]: A tuple containing the index matrix and the number of unique distance values.
    """
    # Initialize a distance matrix of the specified dimension.
    dist_Mat = torch.zeros((dim, dim))

    # Central point coordinates assuming dim is odd.
    i0, j0 = (dim - 1) // 2, (dim - 1) // 2

    # Calculate the Euclidean distance from the center to each point in the grid.
    for i in range(dim):
        for j in range(dim):
            distance = math.sqrt((i0 - i) ** 2 + (j0 - j) ** 2)
            dist_Mat[i, j] = distance

    # Round distances for consistent unique identification.
    dist = torch.round(1000000 * dist_Mat)
    unique = torch.unique(dist.flatten(), dim=-1)

    # Create an index matrix where each element is the index of its distance in the unique list of distances.
    idxs = torch.zeros(dist.shape)
    dimu = unique.shape[0]
    for i in range(dimu):
        mask = (dist == unique[i])
        idxs[mask] = i

    return idxs.long(), dimu


def grid_dist_to_idx2(dim=4):
    """
    Compute the index matrix for a local graph self-attention (GSA) layer in a convolutional neural network,
    based on the distances from the center of a grid, intended to speed up compute.

    Args:
        dim (int): Dimension of the square grid. For correct symmetry inference, dim is typically even.

    Returns:
        Tuple[Tensor, int]: A tuple containing the index matrix and the number of unique distance values.
    """
    # Initialize a distance matrix of the specified dimension.
    dist_Mat = torch.zeros((dim, dim))

    # Calculate the center point's coordinates assuming dim is even.
    i0, j0 = (dim - 1) / 2, (dim - 1) / 2

    # Calculate the Euclidean distance from the center to each point in the grid.
    for i in range(dim):
        for j in range(dim):
            distance = math.sqrt((i0 - i) ** 2 + (j0 - j) ** 2)
            dist_Mat[i, j] = distance

    # Round distances for consistent unique identification.
    dist = torch.round(1000000 * dist_Mat)
    unique = torch.unique(dist.flatten(), dim=-1)

    # Create an index matrix where each element is the index of its distance in the unique list of distances.
    idxs = torch.zeros(dist.shape)
    dimu = unique.shape[0]
    for i in range(dimu):
        mask = (dist == unique[i])
        idxs[mask] = i

    return idxs.long(), dimu


class Sym_Break_Linear(nn.Module):
    """
    A layer that implements a position-invariant linear transformation, but breaks translational symmetry,
    tailored for use in cases where data is embedded on a grid and symmetry considerations are crucial.
    """

    def __init__(self, num_patches, in_features):
        """
        Initialize the layer with the number of patches and the number of input features.

        Args:
            num_patches (int): Total number of patches in the input.
            in_features (int): Number of input features per patch.
        """
        super().__init__()
        self.num_patches = num_patches
        self.idim = int(math.sqrt(num_patches))  # Assumes square grid
        self.in_features = in_features
        # Define the kernel size and padding based on grid dimensions
        self.kernel_size = self.idim + 1
        self.padding = self.idim // 2

        # Get indices from the symmetry-breaking distance matrix
        idxsW, dimW = grid_dist_to_idx(dim=self.kernel_size)
        self.idxsW = idxsW
        self.dimW = dimW

        # Parameter for weights
        self.weights = nn.Parameter(torch.Tensor(in_features, dimW))
        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))  # Initialize weights

        # A linear projection layer
        self.proj = nn.Linear(num_patches - 1, num_patches - 1)

    def forward(self, x):
        """
        Forward pass of the layer.

        Args:
            x (Tensor): Input tensor of shape (batch_size, num_patches, in_features, height, width)

        Returns:
            Tensor: Output tensor after processing.
        """
        bs, np, h, hd = x.size()
        # Select weights based on distance indices, reshape for convolution
        W = torch.index_select(self.weights, 1, self.idxsW.flatten().to(x.device)).reshape(
            (self.in_features, 1, self.kernel_size, self.kernel_size))
        # Apply convolution, expect for Token dimension
        y = F.conv2d(x[:, 1:].transpose(1, 3).reshape(bs * hd, h, self.idim, self.idim), W, padding=self.padding,
                     groups=h)
        # Project output and reshape
        y = self.proj(y.reshape(bs, hd, h, np - 1)).transpose(1, 3)
        # Concatenate with the first channel of input, i.e the token dimension
        return torch.cat([x[:, 0:1].permute(0, 2, 1, 3), y], dim=2)


class Rotation_Symmetry_Break2(nn.Module):
    """
    A custom neural network layer that explicitly handles and breaks flip invariance
    (and preserved rotation symmetry) in input data, using learned angular relationships.
    """

    def __init__(self, num_patches, num_heads):
        """
        Initialize the layer to handle flip invariance with specified patch numbers and head counts.

        Args:
            num_patches (int): Number of patches.
            num_heads (int): Number of attention heads.
        """
        super().__init__()
        self.num_patches = num_patches

        # Generate indices and angles for breaking flip invariance
        idxs, idxs_angles, angles, dim_a = rot_grid_to_idx(int(math.sqrt(num_patches)))
        self.idxs = idxs
        self.angles = nn.Parameter(torch.Tensor(num_heads, dim_a), requires_grad=True)
        self.idxs_angles = idxs_angles
        nn.init.kaiming_uniform_(self.angles, a=math.sqrt(5))

    def forward(self, x):
        """
        Define the forward pass applying the symmetry breaking transformations.

        Args:
            x (Tensor): Input tensor of shape (batch_size, num_heads, height, width)

        Returns:
            Tensor: Output tensor with broken flip invariance.
        """
        bs, heads, ps, _ = x.size()

        # Process input tensor, omitting the first patch and reindexing the remainder, omitt for Token dimension
        y = x[:, :, 1:, 1:].flatten(-2)[:, :, self.idxs]
        angles = torch.index_select(self.angles, 1, self.idxs_angles.flatten().to(x.device))
        y = (y * angles.view(1, heads, 3 * self.num_patches ** 2)).reshape((bs, heads, ps - 1, ps - 1, 3)).sum(-1)
        # Concatenate the modified tensor with the original first slice,  i.e the token dimension
        y = torch.cat([x[:, :, :, 0:1], torch.cat([x[:, :, 0:1, 1:], y], dim=2)], dim=-1)

        return y.reshape(x.shape)


# ------------------------------- Local GSA Block  ----------------------------------------------
# -----------------------------------------------------------------------------

# This is the local graph block, currenlty implementation does not allow for crreating model automatically.

class Sym_Break_Linear_Block(nn.Module):
    """
    Implements a custom local SiT layer that uses convolutional layers with symmetric weights.
    This structure aims to speed up computation by exploiting symmetrical properties of the input data.
    """

    def __init__(self, img_size, patch_size, in_features, num_patches, num_heads):
        """
        Initialize the local SiT layer.

        Args:
            img_size (int): The dimension of the images (assuming square images).
            patch_size (int): The size of each patch (square).
            in_features (int): The number of input features per patch.
            num_patches (int): The total number of patches per image.
            num_heads (int): The number of attention heads in the transformer model.
        """
        super().__init__()
        self.idim = img_size  # Image dimension
        self.patch_size = patch_size  # Size of each patch
        self.in_features = in_features  # Number of input features
        self.num_patches = num_patches  # Total number of patches

        # Define kernel sizes and padding for convolutional layers
        self.kernel_size = self.patch_size + 1
        self.kernel_size0 = self.patch_size // 2 + 1
        self.padding0 = self.patch_size // 4
        self.padding = (self.kernel_size - 1) // 2

        # Get indices for weight sharing based on distance matrices
        idxsW0, dimW0 = grid_dist_to_idx(dim=self.kernel_size0)
        self.idxsW0 = idxsW0
        self.dimW0 = dimW0
        idxsW, dimW = grid_dist_to_idx(dim=self.kernel_size)
        self.idxsW = idxsW
        self.dimW = dimW

        # Initialize weights with different scopes and dimensions
        self.weights = nn.Parameter(torch.Tensor(in_features // 4, dimW))
        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))
        self.weights0 = nn.Parameter(torch.Tensor(in_features // 4, dimW0))
        nn.init.kaiming_uniform_(self.weights0, a=math.sqrt(5))
        self.weights1 = nn.Parameter(torch.Tensor(in_features // 4, dimW))
        nn.init.kaiming_uniform_(self.weights1, a=math.sqrt(5))
        self.weightsa = nn.Parameter(torch.Tensor(in_features // 2, dimW))
        nn.init.kaiming_uniform_(self.weightsa, a=math.sqrt(5))
        self.weights1a = nn.Parameter(torch.Tensor(in_features // 2, dimW))
        nn.init.kaiming_uniform_(self.weights1a, a=math.sqrt(5))

        # Further weights for different layers and attention mechanisms
        idxsW2, dimW2 = grid_dist_to_idx2(dim=self.patch_size // 2)
        self.idxsW2 = idxsW2
        self.dimW2 = dimW2
        self.weights2 = nn.Parameter(torch.Tensor(in_features // 2, dimW2))
        nn.init.kaiming_uniform_(self.weights2, a=math.sqrt(5))

        # Linear projection layers
        self.proj = nn.Linear(in_features // 4, in_features // 4, bias=False)
        self.projk = nn.Linear(in_features // 4, in_features // 2, bias=False)
        self.projq = nn.Linear(in_features // 4, in_features // 2, bias=False)
        self.projx = nn.Linear(in_features // 4, in_features // 2, bias=False)
        self.norm = nn.LayerNorm(3 * (in_features // 2))
        self.projk2 = nn.Linear(in_features // 2, in_features // 2, bias=False)
        self.projq2 = nn.Linear(in_features // 2, in_features // 2, bias=False)
        self.projx2 = nn.Linear(in_features // 2, in_features // 2, bias=False)
        self.norm2 = nn.LayerNorm(3 * (in_features // 2))

        # Attention layers for handling different aspects of data processing
        self.attn = Attention_Basic(in_features // 2, num_patches=num_patches, num_heads=num_heads // 2, qkv_bias=False)
        self.attn2 = Attention_Basic(in_features // 2, num_patches=num_patches, num_heads=num_heads // 2,
                                     qkv_bias=False)

        # Normalization and final projection layers for output processing
        norm_layer = partial(nn.LayerNorm, eps=1e-6)
        self.norm_f = norm_layer(in_features // 2)
        self.projf = nn.Linear(in_features // 2, 16)
        self.projf2 = nn.Linear(16, 2)

    def forward(self, x):
        """
        Defines the forward pass of the Sym_Break_Linear_Block.

        Args:
            x (Tensor): The input tensor with shape (batch_size, features, height, width).

        Returns:
            Tensor: The output tensor after processing through local GSA blocks and convolutional layers.
        """

    def get_patches_flat(self, x, ps):
        """
        Extracts flattened patches from the input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).
            ps (int): Patch size to unfold in both height and width.

        Returns:
            torch.Tensor: Flattened patches of size (batch_size * num_patches, patch_size**2, channels).
        """
        bs, h, w, c = x.size()
        # Unfold extracts sliding local blocks from a batched input tensor
        patches = x.unfold(1, ps, ps).permute(0, 1, 4, 2, 3)
        patches = patches.unfold(3, ps, ps).permute(0, 1, 3, 2, 5, 4)
        return patches.reshape((-1, ps ** 2, c))

    def reconstruct_image(self, patches, ps=4, img_dim=32):
        """
        Reconstructs the image from its patches.

        Args:
            patches (torch.Tensor): Flattened patches.
            ps (int): Patch size used to unfold the patches.
            img_dim (int): The dimension of the square image to reconstruct.

        Returns:
            torch.Tensor: Reconstructed image of shape (batch_size, img_dim, img_dim, channels).
        """
        xy_dim = img_dim // ps
        bs, num_p, num_pn, f = patches.size()
        patches = patches.reshape(bs, num_p, ps, ps, f)
        img = patches.reshape((bs, xy_dim, xy_dim, ps, ps, f)).permute(0, 1, 3, 2, 4, 5)
        img = img.reshape((bs, img_dim, img_dim, f))
        return img

    def forward(self, x):
        # if self.mult is None:
        bs, f, idim, idim = x.size()
        # Define symmetric weitgh by indexing weights repetedily/ i.e. create shared weigth entries. used in GSA layer with respective symmetry properties
        W0 = torch.index_select(self.weights0, 1, self.idxsW0.flatten().to(x.device)).reshape(
            (self.in_features // 4, 1, self.kernel_size0, self.kernel_size0))
        W = torch.index_select(self.weights, 1, self.idxsW.flatten().to(x.device)).reshape(
            (self.in_features // 4, 1, self.kernel_size, self.kernel_size))
        W1 = torch.index_select(self.weights1, 1, self.idxsW.flatten().to(x.device)).reshape(
            (self.in_features // 4, 1, self.kernel_size, self.kernel_size))
        Wa = torch.index_select(self.weightsa, 1, self.idxsW.flatten().to(x.device)).reshape(
            (self.in_features // 2, 1, self.kernel_size, self.kernel_size))
        W1a = torch.index_select(self.weights1a, 1, self.idxsW.flatten().to(x.device)).reshape(
            (self.in_features // 2, 1, self.kernel_size, self.kernel_size))
        W2 = torch.index_select(self.weights2, 1, self.idxsW2.flatten().to(x.device)).reshape(
            (1, self.in_features // 2, (self.patch_size // 2) ** 2)).permute(0, 2, 1)

        # Initial convolutional layer to reduce dimensionality of the problem by Max-Pool below
        y = F.conv2d(x.reshape(bs, f, self.idim, self.idim), W0, padding=self.padding0, groups=f)
        y = nn.ReLU()(self.proj(y.permute(0, 2, 3, 1))).permute(0, 3, 1, 2)
        x = nn.MaxPool2d((2, 2), stride=(2, 2))(y)

        # # Apply the first Graph Self-Attention (GSA) layer
        q = F.conv2d(x.reshape(bs, f, self.idim // 2, self.idim // 2), W, padding=self.padding, groups=f)
        q = self.projq(q.permute(0, 2, 3, 1))  # y.permute(0,2,3,1) #
        k = F.conv2d(x.reshape(bs, f, self.idim // 2, self.idim // 2), W1, padding=self.padding, groups=f)
        k = self.projk(k.permute(0, 2, 3, 1))  # y.permute(0,2,3,1) #
        qkv = self.norm(self.get_patches_flat(torch.cat([q, k, self.projx(x.permute(0, 2, 3, 1)), ], dim=-1),
                                              ps=self.patch_size // 2))
        f = 2 * f
        q, k, v = qkv[:, :, :f], qkv[:, :, f:2 * f], qkv[:, :, 2 * f:]
        x = v + self.attn(q, k, v)
        x = self.reconstruct_image(
            x.reshape(bs, ((self.idim // 2) // (self.patch_size // 2)) ** 2, (self.patch_size // 2) ** 2, f),
            ps=self.patch_size // 2, img_dim=self.idim // 2)
        x = x.permute(0, 3, 1, 2)
        # # Apply the second GSA layer ---> reduced attention window size, only conv spans over more batches
        q = F.conv2d(x.reshape(bs, f, self.idim // 2, self.idim // 2), Wa, padding=self.padding, groups=f)
        q = self.projq2(q.permute(0, 2, 3, 1))  # y.permute(0,2,3,1) #
        k = F.conv2d(x.reshape(bs, f, self.idim // 2, self.idim // 2), W1a, padding=self.padding, groups=f)
        k = self.projk2(k.permute(0, 2, 3, 1))  # y.permute(0,2,3,1) #
        qkv = self.norm2(self.get_patches_flat(torch.cat([q, k, self.projx2(x.permute(0, 2, 3, 1)), ], dim=-1),
                                               ps=self.patch_size // 2))
        q, k, v = qkv[:, :, :f], qkv[:, :, f:2 * f], qkv[:, :, 2 * f:]
        x = v + self.attn2(q, k, v)

        # computing invariant and equivariant contributions
        x_inv = (nn.GELU()(x) * W2).sum(1).reshape(bs, self.num_patches, f)
        x = (nn.GELU()(self.norm_f(x)))
        x = self.projf2(nn.GELU()(self.projf(x)))
        x = x.reshape(bs, self.num_patches, f)
        # concatinating invariant and equivariant contribution
        # for local equivariance only choose x
        # for local invariance only choose x_inv
        x = torch.cat([x, x_inv], dim=-1)

        return x

    # --------------------------------- GSA == Incorporating symmetry breaking layer into Attention --------------------------------------------


# -----------------------------------------------------------------------------


class Attention(nn.Module):
    def __init__(self, dim, num_patches, num_heads=8, qkv_bias=False, qk_scale=None, use_soft=True):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.qkv_SymBreak = Sym_Break_Linear(num_patches, num_heads)
        self.qkv_SymBreak2 = Sym_Break_Linear(num_patches, head_dim)

        # add the follwoing lines for adding flip/rotation symmetry breaking/preserving layer
        # self.rot_SymBreak = Rotation_Symmetry_Break(num_patches,num_heads)
        # self.rot_SymBreak2 = Rotation_Symmetry_Break2(num_patches,num_heads)
        # self.rot_SymBreak3 = Rotation_Symmetry_Break2(num_patches,num_heads)
        self.use_soft = use_soft
        self.proj = nn.Linear(dim, dim)

    def forward(self, x):
        B, N, C = x.shape
        # print("B, N, C", B, N, C)
        qkv = self.qkv(x)
        q = self.qkv_SymBreak(qkv[:, :, :C].reshape(B, N, self.num_heads, C // self.num_heads))  # .permute(0, 2, 1, 3)
        k = self.qkv_SymBreak2(
            qkv[:, :, C:2 * C].reshape(B, N, self.num_heads, C // self.num_heads).transpose(-1, -2)).transpose(1, 3)

        v = qkv[:, :, 2 * C:].reshape(B, N, self.num_heads, C // self.num_heads).transpose(1, 2)

        # v =  qkv[:,:,2*C:].reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        attn = (q @ k.transpose(-1, -2)) * self.scale

        attn = attn + attn.transpose(-2, -1)
        # add the follwoing lines for adding flip/rotation symmetry breaking/preserving layer
        # repeated applicaiotn of layer improves traininig accuracy
        # attn = self.rot_SymBreak(attn)
        # attn = self.rot_SymBreak2(nn.GELU()(attn))- attn
        # attn = self.rot_SymBreak3(nn.GELU()(attn))- attn
        attn = attn.softmax(dim=-1)
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)

        x = self.proj(x)
        # print("x out" , x.shape)
        return x


class Attention_Basic(nn.Module):
    def __init__(self, dim, num_patches, num_heads=8, qkv_bias=False, qk_scale=None, use_soft=True):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # add the follwoing lines for adding flip/rotation symmetry breaking/preserving layer
        # self.rot_SymBreak = Rotation_Symmetry_Break(num_patches,num_heads)
        # self.rot_SymBreak2 = Rotation_Symmetry_Break(num_patches,num_heads)

        # self.use_soft = use_soft
        self.proj = nn.Linear(dim, dim)

    def forward(self, q, k, v):
        B, N, C = v.shape
        q = q.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        k = k.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        v = v.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)

        attn = (q @ k.transpose(-1, -2)) * self.scale
        attn = attn + attn.transpose(-2, -1)
        # add the follwoing lines for adding flip/rotation symmetry breaking/preserving layer
        # attn = self.rot_SymBreak(attn)
        # attn = self.rot_SymBreak2(nn.GELU()(attn)) - attn
        attn = attn.softmax(dim=-1)
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)

        x = self.proj(x)
        # print("x out" , x.shape)
        return x


class Block(nn.Module):
    def __init__(self, dim, num_heads, num_patches, mlp_ratio=1., qkv_bias=False, qk_scale=None,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_soft=True):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.act = act_layer()
        self.attn = Attention(dim, num_patches=num_patches, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,
                              use_soft=use_soft)

    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        return x


class VisionTransformer(nn.Module):
    def __init__(self, num_patches, in_chans=3, embed_dim=64, depth=4,
                 num_heads=8, mlp_ratio=4., qkv_bias=True, qk_scale=None, use_soft=True):
        super().__init__()
        self.embed_dim = embed_dim
        norm_layer = partial(nn.LayerNorm, eps=1e-6)

        self.proj = nn.Linear(embed_dim, 16, bias=False)
        self.proj2 = nn.Linear(16, 4, bias=False)
        self.projf = nn.Linear(embed_dim, 48 // 4)
        self.projf2 = nn.Linear(embed_dim, embed_dim // 4)
        self.blocks = nn.Sequential(*[
            Block_base(
                dim=embed_dim, num_heads=num_heads, num_patches=num_patches, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                norm_layer=norm_layer, act_layer=nn.GELU, use_soft=use_soft)
            for i in range(depth)])
        self.norm = norm_layer(embed_dim)

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        trunc_normal_(self.cls_token, std=.02)
        self.apply(_init_vit_weights)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token'}

    def forward(self, x):
        cls_token = self.cls_token.expand(x.size(0), -1, -1)
        x = torch.cat((cls_token, x), dim=1)
        x = self.blocks(x)
        x = self.norm(x)
        # Combining invariant and equivariant outputs
        y = self.proj2(nn.GELU()(self.proj(x[:, 1:].transpose(-1, -2)))).transpose(-1, -2)  # .flatten(-2)
        y = torch.cat([self.projf(nn.GELU()(y)), self.projf2(nn.GELU()(x[:, 0:1])).reshape(-1, 4, 4)], dim=-1).flatten(
            1)
        return y  # x[:, 0]+


def _init_vit_weights(m, n: str = '', head_bias: float = 0.):
    if isinstance(m, nn.Linear):
        if n.startswith('head'):
            nn.init.zeros_(m.weight)
            nn.init.constant_(m.bias, head_bias)
        elif n.startswith('pre_logits'):
            lecun_normal_(m.weight)
            nn.init.zeros_(m.bias)
        else:
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LayerNorm):
        nn.init.zeros_(m.bias)
        nn.init.ones_(m.weight)


# --------------------------------- SiT/SieT/SeT == putting together lcoal and globa GSA blocks -----------------------
# Combing local and global GSA into Sit model - here a Tiny version with reduced kernel sizes etc... --------------------------------------------
# -----------------------------------------------------------------------------

class SietTiny(nn.Module):
    def __init__(self, img_size=64, patch_size=8, patch_size_local=8, in_chans=3, embed_dim=32, depth=1,
                 num_heads=8, mlp_ratio=4., qkv_bias=True, qk_scale=None, use_soft=True):
        super().__init__()
        self.patch_size = patch_size
        self.patch_size_l = patch_size_local
        self.embed_dim = embed_dim

        img_size_c = img_size
        self.num_patches = (img_size_c // patch_size) ** 2
        self.graph_block = Sym_Break_Linear_Block(img_size=img_size_c, patch_size=patch_size, in_features=embed_dim,
                                                  num_patches=self.num_patches, num_heads=num_heads)
        self.vit_global = VisionTransformer(num_patches=self.num_patches, in_chans=embed_dim, embed_dim=embed_dim,
                                            depth=depth,
                                            num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
                                            qk_scale=qk_scale)

        self.proj = nn.Linear(in_chans, self.embed_dim // 4)
        self.patch_idxs = self.create_patching_idxs(dim=img_size, ps=patch_size_local, ps2=patch_size)  # .flatten()

    def create_patching_idxs(self, dim=32, ps=3 * 4, ps2=4):
        pad_dim = ps2  # int((ps-1)/2)
        image_dim_pad = (2 * pad_dim + dim)
        arr = torch.tensor(list(range(image_dim_pad ** 2))).reshape((image_dim_pad, image_dim_pad)).long()
        idxs = torch.zeros((dim // ps2, dim // ps2, ps, ps)).long()
        for i in range(dim // ps2):
            for j in range(dim // ps2):
                idxs[i, j, :, :] = arr[i:i + ps, j:j + ps]

        return idxs.flatten()

    def get_patches_mini(self, x):
        bs, h, w, c = x.shape
        padding = int((self.patch_size_l) // 3)  # padding = 3*8//2 =3*4 = 4
        x = torch.cat([x[:, ::2, ::2], x[:, 1::2, 1::2], x[:, 1::2, ::2], x[:, ::2, 1::2]], dim=-1)
        y = F.pad(x.permute(0, 3, 1, 2), (padding, padding, padding, padding), mode='constant', value=0)
        y = torch.index_select(y.flatten(-2), 2, self.patch_idxs.to(x.device))
        # print(y.shape)
        y = y.reshape(bs, 4 * c, self.patch_size, self.patch_size, self.patch_size_l, self.patch_size_l)

        return y.permute(0, 2, 3, 4, 5, 1).reshape((-1, self.patch_size_l * self.patch_size_l, 4 * c))  #

    def get_patches_flat(self, x, ps):
        bs, h, w, c = x.size()
        patches = x.unfold(1, ps, ps).permute(0, 1, 4, 2, 3)
        patches = patches.unfold(3, ps, ps).permute(0, 1, 3, 2, 5, 4)
        return patches.reshape((-1, ps ** 2, c))  #

    def forward(self, x):
        t = None

        x = x.permute(0, 2, 3, 1)
        bs, h, w, c = x.shape
        x = self.proj(x)
        x = x.permute(0, 3, 1, 2)
        x = self.graph_block(x).reshape(bs, self.num_patches, self.embed_dim)

        x = self.vit_global(x)
        return x


